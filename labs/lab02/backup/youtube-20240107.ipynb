{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakobzhao/geog458/blob/master/labs/lab02/youtube2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVJiWG0ZQ35Z"
      },
      "outputs": [],
      "source": [
        "# created on April 14, 2021\n",
        "# modified on Jan 2, 2022\n",
        "# modified on April 20, 2023\n",
        "# modified on January 5, 2024\n",
        "# @author:          Bo Zhao\n",
        "# @email:           zhaobo@uw.edu\n",
        "# @website:         https://hgis.uw.edu\n",
        "# @organization:    Department of Geography, University of Washington, Seattle\n",
        "# @description:     A demo of collecting data from YouTube."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VMonPRy4f6J",
        "outputId": "e222365e-b8b4-4204-91f0-f1d9471e8b26"
      },
      "outputs": [],
      "source": [
        "# Install and configure selenium on Google Colab that is built on Ubuntu 18.04. \n",
        "# To install selenium, you will need to update ubuntu, install some dependencies, and install chromedriver.\n",
        "# selenium is a Python package that allows you to automate your browser.\n",
        "# chromedriver is a program that allows you to control Chrome from Python.\n",
        "# chromedriver_autoinstaller is a Python package that automatically installs chromedriver; it can help you determine which version of chromedriver to install based on your version of Chrome.\n",
        "# %%shell means that the following code is a shell script.\n",
        "%%shell\n",
        "sudo apt -y update\n",
        "sudo apt install -y wget curl unzip\n",
        "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
        "dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
        "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "dpkg -i google-chrome-stable_current_amd64.deb\n",
        "\n",
        "wget -N https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/120.0.6099.199/linux64/chromedriver-linux64.zip -P /tmp/\n",
        "unzip -o /tmp/chromedriver-linux64.zip -d /tmp/\n",
        "chmod +x /tmp/chromedriver-linux64/chromedriver\n",
        "mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver\n",
        "pip install selenium chromedriver_autoinstaller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chromedriver_autoinstaller # Importing this package automatically downloads the correct version of chromedriver that matches the installed version of Chrome.\n",
        "chromedriver_autoinstaller.install() # Check if the current version of chromedriver exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_p11hzDF_B7"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "chrome_options = Options() # Create an instance of Options so you can add arguments to the driver.\n",
        "chrome_options.add_argument('--headless') # Add an argument 'headless' to run Chrome in headless mode.\n",
        "chrome_options.add_argument('--no-sandbox') # Add an argument 'no-sandbox' to run Chrome in no-sandbox mode.\n",
        "chrome_options.add_argument('--disable-dev-shm-usage') # Add an argument 'disable-dev-shm-usage' to run Chrome in disable-dev-shm-usage mode.\n",
        "\n",
        "bot = webdriver.Chrome(options=chrome_options) # Create an instance of Chrome. Pass the argument 'options' to the constructor of Chrome.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOJwgbYBRed3"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup # Import BeautifulSoup to parse the HTML.\n",
        "import time, datetime # Import time and datetime to record the time.\n",
        "import pandas as pd # Import pandas to create a dataframe, and it can save the dataframe as a csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj2ClnTbRGp7",
        "outputId": "183101d0-195a-4ae3-fd27-7615fd51613d"
      },
      "outputs": [],
      "source": [
        "# The url where the data will be collected from.\n",
        "url = \"https://www.youtube.com/results?search_query=standing+rock\"\n",
        "\n",
        "# Input the targeting url to the bot, and the bot will load data from the url.\n",
        "bot.get(url)\n",
        "\n",
        "# An array to store all the video urls. If a video has been crawled, it would not be stored to the data frame.\n",
        "video_urls = []\n",
        "# An array to store the retrieved video details.\n",
        "results = []\n",
        "\n",
        "\n",
        "# variable i indicates the number of times that scrolls down a web page. In practice, you might want to develop different\n",
        "# interaction approach to load and view the web pages.\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "    # Create a document object model (DOM) from the raw source of the crawled web page.\n",
        "    # Since you are processing a html page, 'html.parser' is chosen.\n",
        "    soup = BeautifulSoup(bot.page_source, 'html.parser')\n",
        "\n",
        "    # Capture all the video items using find_all or findAll method.\n",
        "    # To view the information of the html elements you want to collect, you need to inspect the raw source using Chrome Inspector.\n",
        "    # To test whether you find the right html elements, you can use the pycharm debugger to examine the returned data.\n",
        "    videos = soup.find_all('ytd-video-renderer', class_=\"style-scope ytd-item-section-renderer\")[-20:] # 20 indicates only process the newly-acquired 20 entries.\n",
        "\n",
        "    # iterate and process each video entry.\n",
        "    for video in videos:\n",
        "\n",
        "        # I prefer use the \"try-except\" statement to enable the program run without pausing due to unexpected errors.\n",
        "        try:\n",
        "            # extract the video url, user url, username, title, view number, created time, short description, and collected time.\n",
        "            # To determine the html elements, you need to inspect the raw source using Chrome Inspector. You can also use ChatGPT to find the html elements.\n",
        "            video_url = video.find(\"a\", class_=\"yt-simple-endpoint style-scope ytd-video-renderer\").attrs[\"href\"]\n",
        "            user_url = video.find(\"a\", class_=\"yt-simple-endpoint style-scope yt-formatted-string\").attrs[\"href\"]\n",
        "            username = video.find(\"a\", class_=\"yt-simple-endpoint style-scope yt-formatted-string\").text\n",
        "            title = video.find(\"yt-formatted-string\", class_=\"style-scope ytd-video-renderer\").text\n",
        "            metadata_items = video.find_all(\"span\", class_=\"inline-metadata-item style-scope ytd-video-meta-block\")\n",
        "            view_num = metadata_items[0].text.replace(\" views\", \"\")\n",
        "            created_at = metadata_items[1].text.replace(\" ago\", \"\")\n",
        "            shortdesc = video.find(\"yt-formatted-string\", class_=\"metadata-snippet-text style-scope ytd-video-renderer\").text\n",
        "            collected_at = datetime.datetime.now()\n",
        "\n",
        "            # create a row in the dict format.\n",
        "            row = {'video_url': video_url,\n",
        "                    'user_url': user_url,\n",
        "                    'username': username,\n",
        "                    'title': title,\n",
        "                    'view_num': view_num,\n",
        "                    'created_at': created_at,\n",
        "                    'shortdesc': shortdesc,\n",
        "                    'collected_at': collected_at}\n",
        "\n",
        "            # if a video has been added, this video would not be inserted to the results array,\n",
        "            # otherwise, this video will be inserted.\n",
        "            if video_url in video_urls:\n",
        "                print(\"this video has already been added.\")\n",
        "            else:\n",
        "                print(row)\n",
        "                video_urls.append(video_url)\n",
        "                results.append(row)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # it is very important to enable the bot take some rest, and then resume to work. \n",
        "    # It will make this data collection process more human-like, otherwise, the bot will be blocked by the website.\n",
        "    time.sleep(5)\n",
        "\n",
        "    # Let the bot scrolls down to the bottom of the content element, most of the time the bot needs to scroll down to the bottom of the page.\n",
        "    bot.execute_script('window.scrollTo(0,  document.getElementById(\"content\").scrollHeight);')\n",
        "\n",
        "# terminate the bot object.\n",
        "bot.close()\n",
        "\n",
        "# Store the results as a pandas dataframe\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# notify the completion of the crawling in the console.\n",
        "print(\"the crawling task is finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3I9cWYBRI8r",
        "outputId": "721a72c6-1fcc-495b-e3ed-b571536f0b99"
      },
      "outputs": [],
      "source": [
        "# Create data on to Google Drive\n",
        "from google.colab import drive\n",
        "# Mount your Drive to the Colab VM.\n",
        "#drive._mount('/gdrive')\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# the file path where to store the output csv on google drive\n",
        "output_file = '/gdrive/My Drive/videos.csv'\n",
        "\n",
        "# Save the dataframe as a csv file\n",
        "df.to_csv(output_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ymNL_9hpTJwr",
        "outputId": "9cdfff37-7a65-44c2-918a-25280e2af663"
      },
      "outputs": [],
      "source": [
        "# download the csv to your local computer\n",
        "from google.colab import files\n",
        "files.download(output_file)\n",
        "print(\"the csv has been downloaded to your local computer. The program has been completed successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
